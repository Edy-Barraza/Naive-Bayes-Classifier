{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h2>Na&iuml;ve Bayes and Feature Extraction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The Na&iuml;ve Bayes Classifier is a linear classifier based off of bayes rule. \n",
    "<br>\n",
    "The feauture extraction functions take data in the form of words and return data in the form of feature vectors. \n",
    "<br>\n",
    "When data is in feature vector form, this program can perform any binary classification!\n",
    "<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Naive Bayes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>\n",
    "An estimate of the class probabilities <b>P(Y)</b>, <b><code>naivebayesPY(x,y)</code></b>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naivebayesPY(x,y):\n",
    "    \"\"\"\n",
    "    function [pos,neg] = naivebayesPY(x,y);\n",
    "\n",
    "    Computation of P(Y)\n",
    "    Input:\n",
    "        x : n input vectors of d dimensions (nxd)\n",
    "        y : n labels (-1 or +1) (nx1)\n",
    "\n",
    "    Output:\n",
    "    pos: probability p(y=1)\n",
    "    neg: probability p(y=-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    y = np.concatenate([y, [-1,1]])\n",
    "    n = len(y)\n",
    "  \n",
    "    return np.count_nonzero(y==1)/n,np.count_nonzero(y==-1)/n\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>An estimate of the conditional probabilities <b>P(X|Y)</b>, <b><code>naivebayesPXY</code></b>.\n",
    "<br>\n",
    "Using a <b>multinomial</b> distribution as model, this will return the probability vectors  for all features given a class label.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def naivebayesPXY(x,y):\n",
    "    \"\"\"\n",
    "    function [posprob,negprob] = naivebayesPXY(x,y);\n",
    "    \n",
    "    Computation of P(X|Y)\n",
    "    Input:\n",
    "        x : n input vectors of d dimensions (nxd)\n",
    "        y : n labels (-1 or +1) (nx1)\n",
    "    \n",
    "    Output:\n",
    "    posprob: probability vector of p(x|y=1) (1xd)\n",
    "    negprob: probability vector of p(x|y=-1) (1xd)\n",
    "    \"\"\"\n",
    "    n,d = x.shape\n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    x = np.concatenate([x, np.ones((2,d))])\n",
    "    y = np.concatenate([y, [-1,1]])\n",
    "    \n",
    "    #return sub array with row vectors where y==1 \n",
    "    #x_posy = x[np.flatnonzero(y== 1),:]\n",
    "    #x_negy = x[np.flatnonzero(y==-1),:]\n",
    "    \n",
    "    \n",
    "    feature_counts_posy= np.sum(x[np.flatnonzero(y== 1),:] ,axis=0)\n",
    "    feature_counts_negy= np.sum(x[np.flatnonzero(y==-1),:] ,axis=0)\n",
    "    \n",
    "    return feature_counts_posy/np.sum(feature_counts_posy),feature_counts_negy/np.sum(feature_counts_negy)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>Log ratio, $\\log\\left(\\frac{P(Y=1 | X = xtest)}{P(Y=-1|X= xtest)}\\right)$, using Bayes Rule, in <b><code>naivebayes</code></b>.\n",
    " <br>\n",
    " Result is >1 if Y=1 is the most probable class.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naivebayes(x,y,xtest):\n",
    "    \"\"\"\n",
    "    function logratio = naivebayes(x,y);\n",
    "    \n",
    "    Computation of log P(Y|X=x1) using Bayes Rule\n",
    "    Input:\n",
    "    x : n input vectors of d dimensions (nxd)\n",
    "    y : n labels (-1 or +1)\n",
    "    xtest: input vector of d dimensions (1xd)\n",
    "    \n",
    "    Output:\n",
    "    logratio: log (P(Y = 1|X=xtest)/P(Y=-1|X=xtest))\n",
    "    \"\"\"\n",
    "    pof_posy,pof_negy = naivebayesPY(x,y)\n",
    "    probs_features_posy, probs_features_negy = naivebayesPXY(x,y)\n",
    "    feature_indices= np.flatnonzero(xtest== 1)\n",
    "    \n",
    "    \n",
    "    return np.log(np.prod(probs_features_posy[feature_indices])*pof_posy/(np.prod(probs_features_negy[feature_indices])*pof_negy) )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>Na√Øve Bayes written as a linear classifier in,\n",
    "<b><code>naivebayesCL</code></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naivebayesCL(x,y):\n",
    "    \"\"\"\n",
    "    function [w,b]=naivebayesCL(x,y);\n",
    "    Implementation of a Naive Bayes classifier\n",
    "    Input:\n",
    "    x : n input vectors of d dimensions (nxd)\n",
    "    y : n labels (-1 or +1)\n",
    "\n",
    "    Output:\n",
    "    w : weight vector of d dimensions\n",
    "    b : bias (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    n, d = x.shape\n",
    "    ## fill in code here\n",
    "    pos_prob,neg_prob = naivebayesPXY(x,y)\n",
    "    py_pos,py_neg   = naivebayesPY(x,y)\n",
    "    return (np.log(pos_prob)-np.log(neg_prob) ), (np.log(py_pos)-np.log(py_neg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>\n",
    "<b><code>classifyLinear</code></b>\n",
    "applies a linear weight vector and bias to a set of input vectors and outputs their predictions. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_biased(xi,w,b):\n",
    "    val=np.dot(xi,w)+b\n",
    "    if val>0:\n",
    "        return 1\n",
    "    elif val<0:\n",
    "        return -1\n",
    "    else:\n",
    "        raise ValueError(\"point lies on hyperplane\")\n",
    "def class_unbiased(xi,w):\n",
    "    val=np.dot(xi,w)\n",
    "    if val>0:\n",
    "        return 1\n",
    "    elif val<0:\n",
    "        return -1\n",
    "    else:\n",
    "        raise ValueError(\"point lies on hyperplane\")\n",
    "def classifyLinear(x,w,b=0):\n",
    "    \"\"\"\n",
    "    function preds=classifyLinear(x,w,b)\n",
    "    \n",
    "    Make predictions with a linear classifier\n",
    "    Input:\n",
    "    x : n input vectors of d dimensions (nxd)\n",
    "    w : weight vector (dx1)\n",
    "    b : bias (scalar)\n",
    "    \n",
    "    Output:\n",
    "    preds: predictions (1xn)\n",
    "    \"\"\"\n",
    "    w = w.reshape(-1)\n",
    "    if b is 0:\n",
    "        return np.fromiter((class_unbiased(xi,w) for xi in x),int, count=x.shape[0])\n",
    "    else:\n",
    "        return np.fromiter((class_biased(xi,w,b) for xi in x),int, count=x.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Extraction</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<b><code>hashfeatures(word, B, FIX)</code></b> takes a word and considers substrings of the word of length 0-`FIX`, both from he beggining and end of the name as features. A value `1` is put into an array at an index determined by the feature's hashing.  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hashfeatures(word, B, FIX):\n",
    "    v = np.zeros(B)\n",
    "    for m in range(FIX):\n",
    "        featurestring = \"prefix\" + word[:m]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "        featurestring = \"suffix\" + word[-1*m:]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<b><code>word2features</code></b> reads every name in the given file and converts it into a B-dimensional feature vector.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2features(filename, B, FIX, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "    X : n feature vectors of dimension B, (nxB)\n",
    "    \"\"\"\n",
    "    # read in words\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            words = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        words = filename.split('\\n')\n",
    "    n = len(words)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures(words[i], B, FIX)\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b><code>genTrainFeatures</code></b>, transforms the words into features and loads them into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genTrainFeatures(dimension,fix,class1_file,class2_file):\n",
    "    \"\"\"\n",
    "    function [x,y]=genTrainFeatures\n",
    "    \n",
    "    This function calls the python script \"word2features.py\" \n",
    "    to convert names into feature vectors and loads in the training data. \n",
    "    \n",
    "    \n",
    "    Output: \n",
    "    x: n feature vectors of dimensionality d [d,n]\n",
    "    y: n labels {-1 , +1 }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load in the data\n",
    "    X_class1 = name2features(class1_file, B=dimension, FIX=fix)\n",
    "    X_class2 = name2features(class2_file, B=dimension, FIX=fix)\n",
    "    X = np.concatenate([X_class1, X_class2])\n",
    "    \n",
    "    # Generate Labels\n",
    "    Y = np.concatenate([-np.ones(len(X_class1)), np.ones(len(X_class2))])\n",
    "    \n",
    "    # shuffle data into random order\n",
    "    ii = np.random.permutation([i for i in range(len(Y))])\n",
    "    \n",
    "    return X[ii, :], Y[ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> Testing</h3>\n",
    "You can now test your code with the following interactive name classification script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dimension=130000, fix=6\n",
    "DIMS = 130000\n",
    "test_fix=6\n",
    "class1_file=\"provide file\"\n",
    "class2_file=\"provide file\"\n",
    "\n",
    "print('Loading data ...')\n",
    "X,Y = genTrainFeatures(DIMS,test_fix,class1_file,class2_file)\n",
    "print('Training classifier ...')\n",
    "w,b=naivebayesCL(X,Y)\n",
    "error = np.mean(classifyLinear(X,w,b) != Y)\n",
    "print('Training error: %.2f%%' % (100 * error))\n",
    "\n",
    "while True:\n",
    "    print('Please enter a word>')\n",
    "    da_word = input()\n",
    "    if len(da_word) < 1:\n",
    "        break\n",
    "    xtest = word2features(da_word,B=DIMS,LoadFile=False)\n",
    "    pred = classifyLinear(xtest,w,b)[0]\n",
    "    if pred > 0:\n",
    "        print(\"%s, First Class Word.\\n\" % da_word)\n",
    "    else:\n",
    "        print(\"%s, Second Class Word.\\n\" % da_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Credits</h4>\n",
    "  Parts of this webpage markup were copied from or heavily inspired by Killian Weinberg's wonderful <a href=\"http://www.cs.cornell.edu/courses/cs4780/2018sp/\">Machine Learning Course</a>."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
